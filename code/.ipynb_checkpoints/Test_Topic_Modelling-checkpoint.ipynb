{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Topic Modelling\n",
    "\n",
    "This has to be done on full dataset. Let's see if my computer can handle it. Else we will move this to Google Colab.  \n",
    "\n",
    "I first try to base the topics on the tf-idf vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import time # will be use to choose the faster solution\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer # Normalize samples individually to unit norm.\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import sklearn\n",
    "import pickle # to save models, for instance LDA outputs\n",
    "\n",
    "\n",
    "# NLP:\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD # LSA\n",
    "from sklearn.decomposition import NMF # NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation # LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data:\n",
    "raw_winedata = pd.read_csv(\"../data/winemag-data-190314.csv\").drop(\"Unnamed: 0\", axis=1) # indexes were stored as col\n",
    "initial_number_of_rows = raw_winedata.shape[0]\n",
    "print(raw_winedata.shape)\n",
    "raw_winedata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to keep only varieties that occur more than n times:\n",
    "n = 500\n",
    "variety_counts = raw_winedata.variety.value_counts()\n",
    "winedata = raw_winedata[raw_winedata.variety.isin(variety_counts.index[variety_counts.gt(n)])] # pandas.DataFrame.gt = get greater\n",
    "print(winedata.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we strongly limit the minimal number of occurences for a variety, we still get a large dataset, and computations will be slow on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to keep only province that occur more than n times:\n",
    "n = 500\n",
    "province_counts = winedata.province.value_counts()\n",
    "winedata = winedata[winedata.province.isin(province_counts.index[province_counts.gt(n)])] # pandas.DataFrame.gt = get greater\n",
    "print(winedata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We keep only the varieties and provinces that are occuring more than 500 times, because we want to try to predict those - in a parallel exercise as building the unsupervised learning system - using the text desciption.** *texte en italique*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_specific_stopwords = ['$', ' ', '’s', 'wine', 'winemaker', 'winemaking', 'winery', \n",
    "                            '2020–2030', '2–3', '3–4', '4–5', '5–6', '6–8', \n",
    "                             'château', 'village', 'beaujolais', 'domaine', \n",
    "                             'côte', 'saint', 'village', 'parcel', 'parcels',\n",
    "                             'I', 'flavors',\n",
    "                             'now-2015', 'now-2018', 'now-2025']\n",
    "                        # consider:\n",
    "#                             cabernet, cabernets, douro, nacional,  \n",
    "#                              widely, muscat,\n",
    "#                              willamette, bordeaux, pommard, rioja, barbaresco, chianti]\n",
    "\n",
    "# a list of all wine varieties. this is not perfect as some varieties are compound words. But it's a start.\n",
    "variety_stopwords = winedata.variety.unique().tolist()\n",
    "variety_stopwords = [str(variety).lower() for variety in variety_stopwords] # convert to lowercase\n",
    "variety_stopwords.append(['pinot', 'cabernet', 'cabernets', 'sauvignon', 'grigio', 'sirah' ]) # as frequent and only happens as compound word\n",
    "\n",
    "# a list of all wine provinces. \n",
    "province_stopwords = winedata.province.unique().tolist()\n",
    "province_stopwords = [str(province).lower() for province in province_stopwords] # convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating our tokenizer function:\n",
    "def tokenize_and_clean(description):\n",
    "\n",
    "    # Tokenize:\n",
    "    mytokens = nlp(description)\n",
    "\n",
    "    # Removing stopwords, punctuation and convert to lower_case + AND NUMBERS (or use is_digit?)\n",
    "    mytokens = [token.lower_ for token in mytokens if not token.is_punct and not token.is_stop and not token.like_num and not token.is_digit]\n",
    "    \n",
    "    # remove domain-specific stopwords:\n",
    "    mytokens = [token for token in mytokens if token not in domain_specific_stopwords]\n",
    "            \n",
    "    # remove wine variety occurrences in the descriptions:\n",
    "    mytokens = [token for token in mytokens if token not in variety_stopwords]\n",
    "                \n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not need word vectors here, so we can upload the small English model from spaCy:\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenize_and_clean, # using our custom tokenizer\n",
    "                               ngram_range=(1,1),\n",
    "                               max_df=0.95, # ignore t that have a df higher than max_df (corpus-specific stopwords)\n",
    "                               min_df=10, # ignore terms that have a doc freq lower than threshold.\n",
    "                               max_features=2000\n",
    "                            )\n",
    "# Applying the vectorizer:\n",
    "wine_tfidf = tfidf_vector.fit_transform(winedata.description) # input: the column \"description\"\n",
    "\n",
    "# Getting the word list.\n",
    "terms = tfidf_vector.get_feature_names()\n",
    "\n",
    "print('Done! it took', time.time()-time0, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics:\n",
    "ntopics=5\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10\n",
    "\n",
    "# to print the top n words:\n",
    "topwords = pd.DataFrame(index=range(0,ntopics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linking words to topics\n",
    "def word_topic(tfidf, solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic = tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components = pd.DataFrame(words_by_topic, index=wordlist)\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    \n",
    "    index = np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords = pd.Series(index=index) # initiate a Series where to store the topwords of each topic\n",
    "    fullist=[] # usual code doesn't work\n",
    "    \n",
    "    for column in n_topics:\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords = components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen = sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist = chosen.index + \"  \"+ round(chosen,2).map(str)\n",
    "        \n",
    "        fullist.append(chosenlist)\n",
    "    \n",
    "#     topwords = pd.Series(fullist, index=index)\n",
    "        \n",
    "#         topwords.loc[column] = chosenlist\n",
    "    return(fullist) # (topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "svd = TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "wine_lsa = lsa.fit_transform(wine_tfidf)\n",
    "\n",
    "components_lsa = word_topic(wine_tfidf, wine_lsa, terms)\n",
    "\n",
    "topwords['LSA'] = top_words(components_lsa, n_top_words) \n",
    "\n",
    "print('Done! It took', time.time()-time0, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNMF\n",
    "time0 = time.time()\n",
    "\n",
    "svd = TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "wine_lsa = lsa.fit_transform(wine_tfidf)\n",
    "\n",
    "components_lsa = word_topic(wine_tfidf, wine_lsa, terms)\n",
    "\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "wine_nmf = nmf.fit_transform(wine_tfidf) \n",
    "\n",
    "components_nmf = word_topic(wine_tfidf, wine_nmf, terms)\n",
    "\n",
    "topwords['NMF'] = top_words(components_nmf, n_top_words) \n",
    "    \n",
    "print('Done! It took', time.time()-time0, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,ntopics):\n",
    "    print('topic', i, ':\\nLSA:\\n', topwords.LSA[i], '\\nNMF:\\n', topwords.NMF[i],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "wine_lda = lda.fit(wine_tfidf)\n",
    "\n",
    "print('Done! It took', time.time()-time0, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print topics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis\n",
    "\n",
    "A good topic model will have non-overlapping, fairly big sized blobs for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(wine_lda, wine_tfidf, tfidf_vector, mds='PCoA') # try also mds='tsne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", wine_lda.score(wine_tfidf))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", wine_lda.perplexity(wine_tfidf))\n",
    "\n",
    "print('Done! it took', time.time()-time0, 'sec.')\n",
    "\n",
    "# See model parameters\n",
    "wine_lda.get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wine_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8e36afb8fa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpkl_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/pickle_LDA_10t.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine_lda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wine_lda' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "pkl_filename = \"../data/pickle_LDA_10t.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(wine_lda, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=8, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=0.125,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS TO LOAD A MODEL:\n",
    "# # Load from file\n",
    "# import pickle\n",
    "\n",
    "# pkl_filename = \"../data/pickle_LDA_10t.pkl\"\n",
    "# with open(pkl_filename, 'rb') as file:\n",
    "#     wine_lda = pickle.load(file)\n",
    "    \n",
    "# wine_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try different number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 15min !!\n",
    "time0 = time.time()\n",
    "\n",
    "all_ntopics = [4,5,6,7,8,20]\n",
    "\n",
    "results = pd.DataFrame(all_ntopics, columns=['ntopics'], index=all_ntopics)\n",
    "\n",
    "for ntopics in all_ntopics:\n",
    "    lda = LatentDirichletAllocation(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "    results.loc[ntopics, 'LDA'] = lda.fit(wine_tfidf)\n",
    "    \n",
    "    results.loc[ntopics, 'LL'] = results.loc[ntopics, 'LDA'].score(wine_tfidf)\n",
    "    results.loc[ntopics, 'perplexity'] = results.loc[ntopics, 'LDA'].perplexity(wine_tfidf)\n",
    "\n",
    "print('Done! It took', time.time()-time0, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of results:\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pickle_LDA_4t.pkl\n",
      "../data/pickle_LDA_5t.pkl\n",
      "../data/pickle_LDA_6t.pkl\n",
      "../data/pickle_LDA_7t.pkl\n",
      "../data/pickle_LDA_8t.pkl\n",
      "../data/pickle_LDA_20t.pkl\n"
     ]
    }
   ],
   "source": [
    "# SAVE ALL THESE OUTPUTS !!!\n",
    "\n",
    "for ntopics in all_ntopics:\n",
    "    # Save to file:\n",
    "    pkl_filename = '../data/pickle_LDA_' + str(ntopics) + 't.pkl'\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(results.loc[ntopics, 'LDA'], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[6, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'\n",
    "\n",
    "# Save to file:\n",
    "pkl_filename = \"../data/pickle_LDA_10t.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(wine_lda, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[7, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[8, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[4, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[5, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 topics:\n",
    "pyLDAvis.sklearn.prepare(results.loc[20, 'LDA'], wine_tfidf, tfidf_vector, mds='PCoA') # or mds='mmds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features from Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
